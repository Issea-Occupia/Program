# mnist_start.py
# ========================
# ğŸš€ MNIST æ‰‹å†™æ•°å­—è¯†åˆ«å…¥é—¨ç¨‹åº
# ä½œè€…: ChatGPT & Issea Occupia
# åŠŸèƒ½: è®­ç»ƒä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œè¯†åˆ«æ‰‹å†™æ•°å­— (0~9)
# ========================

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

# ========================
# ä¸€ã€ç¯å¢ƒä¸æ•°æ®å‡†å¤‡
# ========================

# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ (GPU ä¼˜å…ˆ)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ’» Using device: {device}")

# æ•°æ®é¢„å¤„ç†: è½¬æ¢ä¸ºTensor + å½’ä¸€åŒ–åˆ°(-1, 1)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# ä¸‹è½½å¹¶åŠ è½½ MNIST æ•°æ®é›†
train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)

# ========================
# äºŒã€å®šä¹‰ç½‘ç»œç»“æ„
# ========================

class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # è¾“å…¥å±‚ -> éšè—å±‚1
        self.fc2 = nn.Linear(128, 64)       # éšè—å±‚1 -> éšè—å±‚2
        self.fc3 = nn.Linear(64, 10)        # éšè—å±‚2 -> è¾“å‡ºå±‚

    def forward(self, x):
        x = x.view(-1, 28 * 28)  # å±•å¹³å›¾ç‰‡
        x = F.relu(self.fc1(x))  # æ¿€æ´»å‡½æ•° ReLU
        x = F.relu(self.fc2(x))
        x = self.fc3(x)          # è¾“å‡ºå±‚ï¼ˆä¸åŠ Softmaxï¼‰
        return x

# ========================
# ä¸‰ã€è®­ç»ƒå‡†å¤‡
# ========================

model = SimpleNet().to(device)
criterion = nn.CrossEntropyLoss()               # äº¤å‰ç†µæŸå¤±å‡½æ•°
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# ========================
# å››ã€è®­ç»ƒæ¨¡å‹
# ========================

epochs = 50
for epoch in range(epochs):
    running_loss = 0.0
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{epochs}] | Loss: {running_loss/len(train_loader):.4f}")

print("âœ… è®­ç»ƒå®Œæˆï¼")

# ========================
# äº”ã€æ¨¡å‹æµ‹è¯•
# ========================

correct, total = 0, 0
with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"ğŸ¯ æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")

# ========================
# å…­ã€ä¿å­˜æ¨¡å‹
# ========================

torch.save(model.state_dict(), "mnist_simple.pth")
print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º mnist_simple.pth")

# ========================
# ä¸ƒã€ç»“è¯­
# ========================
# ä¸‹ä¸€æ­¥å»ºè®®ï¼š
# 1ï¸âƒ£ ç†è§£ tensorã€åå‘ä¼ æ’­æœºåˆ¶ï¼›
# 2ï¸âƒ£ å°è¯•æ·»åŠ å·ç§¯å±‚ï¼ˆCNNï¼‰ï¼›
# 3ï¸âƒ£ ä½¿ç”¨ matplotlib å¯è§†åŒ–é¢„æµ‹ç»“æœã€‚
